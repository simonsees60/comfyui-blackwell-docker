# ComfyUI + Blackwell NVFP4 Docker Configuration
# Copy this file to .env and customize for your system

# ==============================================================================
# DIRECTORY PATHS
# ==============================================================================
# These are where your files will be stored on your host machine.
# Create these directories before running docker-compose up.

# Where to store AI models (FLUX, SD, etc.) - This can get LARGE (100GB+)
MODELS_PATH=./models

# Where generated images will be saved
OUTPUT_PATH=./output

# Where you can place input images for img2img workflows
INPUT_PATH=./input

# Custom nodes directory - ComfyUI Manager installs go here
CUSTOM_NODES_PATH=./custom_nodes

# User settings and workflows
USER_PATH=./user

# ==============================================================================
# PORT CONFIGURATION
# ==============================================================================
# The port where ComfyUI web interface will be accessible
# Access at http://localhost:8188 after starting
COMFYUI_PORT=8188

# ==============================================================================
# GPU MEMORY SETTINGS
# ==============================================================================
# These control how ComfyUI manages VRAM
# Adjust based on your GPU's VRAM capacity

# Reserve this much VRAM (in GB) for other applications
# Recommended: 1.5-2.0 for 24GB cards, 1.0 for 16GB cards
RESERVE_VRAM=1.5

# Additional startup arguments (space-separated)
# Current defaults enable low VRAM mode and async offloading
# Remove --lowvram if you have 24GB+ VRAM for better performance
COMFYUI_ARGS=--lowvram --async-offload

# ==============================================================================
# PYTORCH WHEEL URLS
# ==============================================================================
# WHY SPECIFIC WHEEL URLS MATTER:
# Pip's dependency resolver will try to "help" by downgrading/upgrading PyTorch
# to satisfy other packages. This BREAKS CUDA compatibility and wastes hours.
# By specifying EXACT wheel URLs, we bypass the resolver and lock versions.
#
# WHERE TO FIND WHEELS:
# https://download.pytorch.org/whl/
#
# FOR torch, torchvision and torchaudio YOU NEED TO:
#
# WHAT YOU NEED TO MATCH:
# 1. CUDA version - Must match your NVIDIA driver (check: nvidia-smi)
#    - Blackwell GPUs need CUDA 12.8+, recommended 13.0+ for comfyui
#    - Look for: cu130 (for CUDA 13.0), cu128 (for CUDA 12.8), etc.
#
# 2. Python version - Must match the base image
#    - The NVida base image uses ubuntu24.04 which ships with Python 3.12
#    - So look for: cp312 (Python 3.12)
#
# 3. Platform - Must be Linux x86_64
#    - Look for: linux_x86_64 (what the NVidia base image ubuntu uses)
#
# 4. The torch and torchaudio have the same version number, but torchvision
# is separate, however they all change in lockstep. If you dont understand
# just ask your favourite AI to explain. Its currently 2.10.0 for
# torch/torchaudio and 0.25.0 for torchvision for example.
#
# EXAMPLE SEARCH PATTERN:
# For CUDA 13.0, Python 3.12:
# torch-2.10.0+cu130-cp312-cp312-manylinux_2_28_x86_64.whl
#
# INSTRUCTIONS:
# 1. Go to https://download.pytorch.org/whl/
# 2. Search for your CUDA version (e.g., "cu130")
# 3. Navigate in to the relevant folder for torch, torchvision and torchaudio
# 3. Find the wheel matching your Python version, cuda version and linux
# 4. Copy the FULL URLs and paste them below
# 5. Make sure the versions are compatible (check PyTorch release notes)

TORCH_WHEEL_URL=https://download.pytorch.org/whl/cu130/torch-2.10.0%2Bcu130-cp312-cp312-manylinux_2_28_x86_64.whl
TORCHVISION_WHEEL_URL=https://download.pytorch.org/whl/cu130/torchvision-0.25.0%2Bcu130-cp312-cp312-manylinux_2_28_x86_64.whl
TORCHAUDIO_WHEEL_URL=https://download.pytorch.org/whl/cu130/torchaudio-2.10.0%2Bcu130-cp312-cp312-manylinux_2_28_x86_64.whl

# ==============================================================================
# SAGEATTENTION VERSION
# ==============================================================================
# SageAttention provides optimized attention mechanisms for transformers
# Latest releases: https://github.com/thu-ml/SageAttention/releases
# PyPI page: https://pypi.org/project/sageattention/
#
# SageAttention requires Triton (installed automatically)
# For Blackwell GPUs, use 2.2.0+ for best performance

SAGEATTENTION_VERSION=2.2.0

# ==============================================================================
# BASE DOCKER IMAGE
# ==============================================================================
# The CUDA development image that provides GPU support and build tools
# Browse available versions: https://hub.docker.com/r/nvidia/cuda/tags
# 
# Format: nvidia/cuda:CUDA_VERSION-devel-ubuntu_VERSION
# Current: CUDA 13.1.0 with Ubuntu 24.04
# 
# IMPORTANT: 
# - Use "devel" images (not "runtime") for building packages
# - Ubuntu version determines Python version (affects wheel selection):
#   • Ubuntu 24.04 = Python 3.12 (cp312)
#   • Ubuntu 22.04 = Python 3.10 (cp310)
#   • Ubuntu 20.04 = Python 3.8 (cp38)
# - For Blackwell GPUs: CUDA 12.8+ minimum, 13.1 recommended
#
# When you change this, you MUST update TORCH_WHEEL_URL, TORCHVISION_WHEEL_URL,
# and TORCHAUDIO_WHEEL_URL to match the new Python version!

CUDA_BASE_IMAGE=nvidia/cuda:13.1.1-devel-ubuntu24.04

# ==============================================================================
# COMFYUI VERSION
# ==============================================================================
# Which branch/tag of ComfyUI to clone
# Options: master (latest), or a specific commit hash for stability
# Leave as 'master' for latest features

COMFYUI_BRANCH=master

# ==============================================================================
# IMPORTANT NOTES
# ==============================================================================
#
# 1. WHEN TO REBUILD THE IMAGE:
#    - You changed ANY version above
#    - You installed custom nodes via ComfyUI Manager
#    - You modified wheels.txt
#    - You modified the Dockerfile - even though you shouldn't need to
#    
#    To rebuild: docker-compose build --no-cache
#
# 2. CUSTOM NODES WORKFLOW:
#    Installing nodes via ComfyUI Manager does NOT automatically install their
#    Python dependencies. You must rebuild the image to bake them in:
#    
#    a) Install nodes through ComfyUI Manager (web UI)
#    b) It will probably stuff up and tell you it didnt work, however it 
#    should clone the git repo - if not do this manually into wherever you
#    setup your custom nodes folder
#    c) Stop container: docker-compose down
#    d) Rebuild: docker-compose build
#    e) Start: docker-compose up -d
#    f) Grab a coffee it takes a while
#    
#    This ensures requirements.txt files are properly installed and your
#    PyTorch versions stay protected.
#
# 3. NUNCHAKU INSTALLATION:
#    Nunchaku wheels are specified in wheels.txt, NOT here.
#    Edit wheels.txt to update Nunchaku version.
#    Find wheels at: https://github.com/nunchaku-ai/nunchaku/releases
#    
#    The wheel filename must match your PyTorch and Python versions:
#    nunchaku-VERSION+torchX.XX-PYTHON_VERSION-PYTHON_VERSION-linux_x86_64.whl
#
# 4. VERSION COMPATIBILITY:
#    Everything must align with the base image:
#    
#    Ubuntu 24.04 → Python 3.12 (cp312)
#         ↓
#    CUDA 13.0 → cu130 wheels
#         ↓
#    Blackwell GPU support
#    
#    If you change the base image, you need to find matching wheel URLs.
#
# ==============================================================================
